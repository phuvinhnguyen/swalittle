environment:
  class: Dont7.Dont7Env
  params: {}
model:
  class: llm.GeneralLLMWrapper
  params:
    model_name_or_path: "HuggingFaceTB/SmolLM2-135M"
algorithm:
  class: base.GRPO
  params:
    gamma: 0.9
trainer:
  log_name: "GRPO_Dont7"
  episodes_per_batch: 10
  report_folder: "reports"
  epochs: 50
  max_step_each_time: [30, 40, 100]